{"cells":[{"metadata":{"_uuid":"1b0751bb-09fc-45da-91e6-d379459c2491","_cell_guid":"7659e214-f21e-4efa-8a6c-08b0fbfc2006","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's get started with Titanic Problem","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# IMPORTING THE LIBRARIES","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IMPORTING THE 'TRAIN' AND 'TEST' DATASETS","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = train.iloc[:, 1].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory Data Analysis ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will explore the given data with various given features in datasets before jumping to modeling the data. Here, our main ojective is to gain as much knowledge as we can. We will have maximum insights of data with library Seaborn.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Analysing data with graphs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Countplot \nsns.catplot(x =\"Sex\", hue =\"Survived\", kind =\"count\", data = train) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After observing the above graph, we can say that women were more likely to survived than men as they have high rate of survival than man. Hence, in determining whether a passenger will survive or not, gender(male or female) plays an important role.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"group = train.groupby(['Pclass', 'Survived']) \npclass_survived = group.size().unstack() \n  \nsns.heatmap(pclass_survived, annot = True, fmt =\"d\") ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It helps in determining if higher-class passengers had more survival rate than the lower class ones or vice versa. Class 1 passengers have a higher survival chance compared to classes 2 and 3. It implies that Pclass contributes a lot to a passenger’s survival rate.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Code : Factor plot for Family_Size (Count Feature) and Family Size.\n\n# Adding a column Family_Size \ntrain['Family_Size'] = 0\ntrain['Family_Size'] = train['Parch']+train['SibSp'] \n  \n# Adding a column Alone \ntrain['Alone'] = 0\ntrain.loc[train.Family_Size == 0, 'Alone'] = 1\n  \n# Factorplot for Family_Size \nsns.factorplot(x ='Family_Size', y ='Survived', data = train) \n  \n# Factorplot for Alone \nsns.factorplot(x ='Alone', y ='Survived', data = train) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Family_Size denotes the number of people in a passenger’s family. It is calculated by summing the SibSp and Parch columns of a respective passenger. Also, another column Alone is added to check the chances of survival of alone passenger against the one with a family.\n\nImportant observations –\n\nIf a passenger is alone, the survival rate is less.\nIf the family size is greater than 5, chances of survival decreases considerably.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Code : Bar Plot for Fare\n\n\n# Divide Fare into 4 bins \ntrain['Fare_Range'] = pd.qcut(train['Fare'], 4) \n  \n# Barplot - Shows approximate values based  \n# on the height of bars. \nsns.barplot(x ='Fare_Range', y ='Survived', data = train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fare denotes the fare paid by a passenger. As the values in this column are continuous, they need to be put in separate bins(as done for Age feature) to get a clear idea. It can be concluded that if a passenger paid a higher fare, the survival rate is more.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['Age'].dropna(), bins=15, kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Many passensgers are of age 15-40 yrs.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#Code : Categorical Count Plots for Embarked Feature\n\n\n# Countplot \nsns.catplot(x ='Embarked', hue ='Survived', kind ='count', col ='Pclass', data = train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Some notable observations are:**\n\n* Majority of the passengers boarded from S. \n\n* Majority of class 3 passengers boarded from Q.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# Overall Conclusions from EDA:","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"\n1. Women survived more than men.\n2. Class 1 passengers were more lucky than Class 2 and 3.\n3. Unfortunately, Class 3 was most affected.\n4. Alone passengers had less survival rate.\n5. Survival rate is more for passengers who paid higher fare.\n6. Most of the passangers were of age between 20-40.\n7. Majority of the passengers were boarded from 'S'.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We are done with EDA. Now, we will perform Data Preprocessing on both train and test dataset followed by Feature Scaling and then finally we will train our datasets on various models.\n\n**Excited......????\n      \n  So let's get one step closer to solve this problem...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First we will drop unnecessary columns because they do not contribute to final output.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_eda_cols = ['SibSp', 'Parch', 'Family_Size', 'Fare_Range', 'Alone']\ntrain = train.drop(extra_eda_cols, axis = 1, inplace = False)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#droping the  unnecessary columns\n\nextra_cols = ['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin']\ntrain = train.drop(extra_cols, axis = 1, inplace = False)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train = train.drop('Survived', axis = 1, inplace = False)\nprint(x_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking the missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(x_train.isnull())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checking how many missing values are there.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"'Age' has 177 and 'Embarked' has 2 missing values.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Filling the missing values column by column using scikit-learn.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#For 'Age' column\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(x_train[['Age']])\nx_train[['Age']]= imputer.transform(x_train[['Age']])\n\n\n#For 'Embarked' column\n\nimputers = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputers.fit(x_train[['Embarked']])\nx_train[['Embarked']]= imputers.transform(x_train[['Embarked']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we see, now there is not any missing value in any column.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding Categorical Data ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \n\n\n#Sex Column  \nx_train['Sex']= label_encoder.fit_transform(x_train['Sex']) \n\n#Embarked Column\nx_train['Embarked']= label_encoder.fit_transform(x_train['Embarked'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Feature Scaling on training data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing on TEST Dataset","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking for missing values","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test.isnull().sum().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(test.isnull())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'Cabin' has maximum number of missing values. 'Age' column also have many NaN values and 'Fare' cloumn has 1 missing value.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Filling missing values column by column","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#For 'Age' column\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(test[['Age']])\ntest[['Age']]= imputer.transform(test[['Age']])\n\n\n#For 'Embarked' column\n\nimputers = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputers.fit(test[['Embarked']])\ntest[['Embarked']]= imputers.transform(test[['Embarked']])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Dropping unnecessary columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"extra_cols_test = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin']\ntest = test.drop(extra_cols_test, axis = 1, inplace = False)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Encoding Categorical Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \n\n\n#Sex Column  \ntest['Sex']= label_encoder.fit_transform(test['Sex']) \n\n#Embarked Column\ntest['Embarked']= label_encoder.fit_transform(test['Embarked'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Applying Feature Scaling on Test Set","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\ntest = sc_x.fit_transform(test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Various Classification Models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"We will build a number of Classification models and at the end we will take the model having highest accuracy.\nSo let's get started........","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Logistic Regression","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(test)\nfrom sklearn.model_selection import cross_val_score\nacc_Tree = cross_val_score(classifier, x_train, y_train, cv=10, scoring='accuracy').mean()\nacc_Tree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KNN","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=5)\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(test)\nfrom sklearn.model_selection import cross_val_score\nacc_Tree = cross_val_score(classifier, x_train, y_train, cv=10, scoring='accuracy').mean()\nacc_Tree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Decision Tree Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier()\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(test)\nfrom sklearn.model_selection import cross_val_score\nacc_Tree = cross_val_score(classifier, x_train, y_train, cv=10, scoring='accuracy').mean()\nacc_Tree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Classifier","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(x_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(test)\nfrom sklearn.model_selection import cross_val_score\nacc_Tree = cross_val_score(classifier, x_train, y_train, cv=10, scoring='accuracy').mean()\nacc_Tree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SVC","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\nclassifier = SVC()\nclassifier.fit(x_train, y_train)\ny_pred = classifier.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(test)\nfrom sklearn.model_selection import cross_val_score\nacc_Tree = cross_val_score(classifier, x_train, y_train, cv=10, scoring='accuracy').mean()\nacc_Tree","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we are done with all the models. Now let's make a dataframe showing models with their cross_val_score for visualizing in a good way.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"accuracy = {'Model' : ['Logistic Regression', 'K- Nearest Neighbor', 'SVC', 'Decision Tree', 'Random Forest'],\n                  'Accuracy' : [0.7890, 0.8047, 0.8226, 0.7935, 0.8037]\n                 }\nall_cross_val_scores = pd.DataFrame(accuracy, columns = ['Model', 'Accuracy'])\nall_cross_val_scores.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Alright as we can see, '**SVC**' has highest score. So, here we have best model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df = pd.read_csv('../input/titanic/test.csv')\nsubmission = pd.DataFrame({\n    'PassengerId': test_df['PassengerId'],\n    'Survived': y_pred\n})\nsubmission.to_csv('titanic_prediction.csv', index=False)\nprint('File Saved')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}